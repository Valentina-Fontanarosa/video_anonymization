## Contesto

La crescente diffusione di sistemi di videosorveglianza, dispositivi mobili e strumenti di registrazione ha portato a un aumento esponenziale dei dati video generati quotidianamente. Tuttavia, questi contenuti spesso includono informazioni sensibili, come volti, targhe e altri elementi che possono violare la privacy degli individui.

In questo contesto, il progetto **`video_anonymization`** nasce con lâ€™obiettivo di sviluppare un sistema automatico per lâ€™anonimizzazione dei video, capace di identificare e oscurare in modo intelligente le aree sensibili allâ€™interno delle immagini.

---

## ðŸŽ¯ Obiettivo del progetto

Lâ€™obiettivo Ã¨ stato quello di progettare e implementare un sistema automatizzato per lâ€™anonimizzazione visiva di parti sensibili, come i volti di soggetti e le targhe di veicoli, nei flussi video urbani raccolti da telecamere di videosorveglianza stradale.

Tale sistema deve garantire la qualitÃ  visiva e tempi di latenza compatibili con lâ€™elaborazione in tempo reale, nel rispetto delle normative europee in materia di privacy e intelligenza artificiale.

Il sistema quindi individua antonomasticamente i soggetti da anonimizzare e in base alla classe semantica rilevata genera una maschera binaria pixel-wise che delimita lâ€™area da modificare e un prompt testuale specifico che descrive il contenuto da rigenerare.

---

## Architettura della pipeline di anonimizzazione

Lâ€™architettura del sistema proposto per lâ€™anonimizzazione si basa su una pipeline modulare progettata per operare su flussi video sfruttando i principi del modello di Diffusione generativo.

Il processo ha inizio con il blocco di **encoding**, in cui lâ€™immagine originale `X` di input viene convertita in uno **spazio latente compresso `Z`** tramite un **autoencoder convoluzionale**. In particolare Ã¨ stato utilizzato un **variational autoencoder** composto da una serie di convoluzioni, *attention block* e *residual block*, progettati per ridurre progressivamente la risoluzione spaziale dellâ€™immagine e aumentare la profonditÃ  semantica.

In parallelo, oltre al VAE, Ã¨ presente un **text encoder basato su CLIP**, che elabora il prompt testuale fornito in input e genera le corrispondenti *prompt embeddings*, che guideranno la generazione del contenuto sostitutivo.

Nel contesto dellâ€™**inpainting**, viene impiegata una **maschera binaria** per definire con precisione le aree dellâ€™immagine da anonimizzare. Questa maschera viene applicata sullâ€™immagine di input per poi essere trasformata nello spazio latente `Z_masked`, aggiungendo **rumore gaussiano**.

Tale latente degradato viene poi passato allo **scheduler**, responsabile della schedulazione temporale del processo di *denoising*. Esso gestisce i passaggi di rimozione progressiva del rumore durante la fase di generazione.

Successivamente, viene coinvolto il **blocco di time embedding**, che integra le informazioni temporali `t` allâ€™interno del processo generativo.

Il cuore del sistema Ã¨ rappresentato dal modulo **U-Net**, che riceve come input: il latente degradato `Z_masked`, la maschera binaria, le *prompt embeddings* e le informazioni temporali. Lâ€™U-Net genera una sequenza di feature che verranno poi utilizzate per ricostruire lâ€™immagine anonimizzata nello spazio visivo.


![Architettura della pipeline](images/inpainting_schema.png)

---

## ðŸ“Š Analisi delle metriche

Per quanto riguarda la valutazione del sistema, ho condotto una serie di test quantitativi. I risultati ottenuti sono riportati nella tabella.

![Valutazione metriche](images/valutazione_metriche.png)

Le metriche utilizzate sono PSNR e SSIM, che misurano la fedeltÃ  visiva e strutturale rispetto allâ€™originale, LPIPS che valuta la similaritÃ  percepita, e FID utilizzata per analizzare quanto la distribuzione delle feature generate fosse simile a quella reale.

Per la categoria Persone, i valori di PSNR e SSIM sono bassi, e LPIPS Ã¨ basso: questo indica che le immagini rigenerate sono coerenti e realistiche. Il valore FID Ã¨ invece elevato, ma in questo contesto Ã¨ positivo: significa che lâ€™identitÃ  visiva Ã¨ stata modificata in modo significativo, come richiesto per lâ€™anonimizzazione.

Per i veicoli i valori sono piÃ¹ contenuti, ma il FID Ã¨ piÃ¹ basso: segno che il sistema ha rigenerato in modo efficace senza alterazioni drastiche, dato che i veicoli richiedono unâ€™alterazione meno invasiva.

In sintesi, le metriche confermano che il sistema garantisce un buon equilibrio tra qualitÃ  visiva e anonimizzazione efficace.

---

## Performance
 
Lâ€™obiettivo Ã¨ stato quello di capire se il sistema puÃ² operare efficacemente in contesti real-time, come la videosorveglianza urbana.  
Per farlo, sono state implementate varie ottimizzazioni: in primo luogo, tutta lâ€™inferenza Ã¨ stata eseguita in precisione FP16, che consente di dimezzare il carico computazionale senza compromettere la qualitÃ .  

Sono state inoltre testate diverse configurazioni hardware, tra cui GPU di fascia media e alta.  
Il tempo medio di elaborazione per frame Ã¨ risultato compatibile con requisiti di near real-time, soprattutto grazie alla parallelizzazione dei processi di encoding e inpainting.  
Ãˆ stata inoltre ridotta la latenza introducendo un sistema di prefetching e ottimizzazione della gestione delle maschere.  

In conclusione, lâ€™architettura proposta riesce a bilanciare efficacemente qualitÃ  dellâ€™anonimizzazione e tempi di esecuzione, risultando adatta a scenari operativi reali.


![Valutazione performance](images/valutazione_performance.png)

---

## ðŸ“ Download necessari

### ðŸ“¥ Tokenizer

Scarica i seguenti file da Hugging Face e salvali nella cartella `data/`:

- [`vocab.json`](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/tokenizer/vocab.json)
- [`merges.txt`](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/tokenizer/merges.txt)

> ðŸ“Œ URL di origine: [https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/tree/main/tokenizer](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/tree/main/tokenizer)

---

### ðŸ“¥ Stable Diffusion Weights

Scarica il checkpoint principale da:

- [`v1-5-pruned-emaonly.ckpt`](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt)

Salvalo nella cartella `data/`.

> ðŸ“Œ URL di origine: [https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/tree/main](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/tree/main)

---

### âœ… Modelli fine-tuned testati

Puoi anche utilizzare modelli fine-tuned basati su Stable Diffusion v1.5. Ad esempio:

- [`v1-5-pruned-emaonly-fp16.safetensors`](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/resolve/main/v1-5-pruned-emaonly-fp16.safetensors)

Salvalo nella cartella `data/`.

> ðŸ“Œ URL di origine: [https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)

---

## ðŸ“¦ YOLOv8-seg (Segmentazione)

Per usare la segmentazione, Ã¨ necessario scaricare YOLOv8 con supporto a segmentazione (`yolov8-seg`)
